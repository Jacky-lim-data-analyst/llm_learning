{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd69d200-198e-4118-8f28-462cf1a8a75b",
   "metadata": {},
   "source": [
    "# Perform fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f386cd5-11f6-405d-8ada-7260ad36c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets) (1.24.1)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.1.4-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ---------------------------------------- 0.0/341.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 341.8/341.8 kB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 521.2/521.2 kB 31.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB ? eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 166.4/166.4 kB 10.4 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.1-cp310-cp310-win_amd64.whl (364 kB)\n",
      "   ---------------------------------------- 0.0/364.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 364.6/364.6 kB 22.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp310-cp310-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.5/24.6 MB 48.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 40.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.7/24.6 MB 37.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 36.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.7/24.6 MB 35.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 8.9/24.6 MB 33.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 32.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.2/24.6 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.4/24.6 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.8/24.6 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.0/24.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.5/24.6 MB 25.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.7/24.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.2/24.6 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.4/24.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading pandas-2.1.4-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.1/10.7 MB 35.7 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.3/10.7 MB 29.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.6/10.7 MB 28.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.8/10.7 MB 27.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.1/10.7 MB 25.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.3/10.7 MB 26.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.7 MB 26.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.7 MB 26.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 26.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.4/44.4 kB ? eta 0:00:00\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "   ---------------------------------------- 0.0/502.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 502.5/502.5 kB 30.8 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB ? eta 0:00:00\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.2\n",
      "    Uninstalling fsspec-2023.12.2:\n",
      "      Successfully uninstalled fsspec-2023.12.2\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.15.0 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.10.0 multidict-6.0.4 multiprocess-0.70.15 pandas-2.1.4 pyarrow-14.0.1 pyarrow-hotfix-0.6 pytz-2023.3.post1 tzdata-2023.3 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773dd697-149f-423a-9622-535cfe9c267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fict\\Documents\\limjq_llm\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# This dataset contains 2 sentence and will be labeled 0 and 1 based on similarity\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f95d15-bb25-4a6e-b387-e2d7130dcbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63e6b10-2dd8-4888-826a-e44b16dc4102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9baba2b6-2e75-4e3a-bb67-2a90932621fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][0]['sentence1'])\n",
    "print(raw_datasets[\"train\"][0]['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e55c9dc-d2b5-4e17-a91a-77c3c1e9f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\n",
      "Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][1]['label'])\n",
    "print(raw_datasets[\"train\"][1]['sentence1'])\n",
    "print(raw_datasets[\"train\"][1]['sentence2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb624700-abaf-4253-9f6e-f4cda5f723a3",
   "metadata": {},
   "source": [
    "So, `1` means same meaning while `0` means different meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdcb95e-b6da-485a-97a2-8669514ea26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████| 408/408 [00:00<00:00, 14557.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# map the data to suitable format for fine-tuning\n",
    "# define the collator as well\n",
    "tokenized_dataset = raw_datasets.map(tokenize_func, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a4778a-10a6-410e-829a-63b64c31e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# now lets define the model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6400badb-e432-469e-88a2-31dbd43795ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from accelerate) (2.1.2+cu118)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "   ---------------------------------------- 0.0/265.7 kB ? eta -:--:--\n",
      "   -------------------------------------- - 256.0/265.7 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 265.7/265.7 kB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f7110d-3cbc-46ce-a8b9-6046c9a8721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\"train_classification1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeca3562-6570-486c-ad6c-824b916c706a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93b60caf-27b5-4ccf-b814-9b67b7ae6920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83a82900-1bdd-48ef-9c16-486355b59c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.279000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.32450293802054964, metrics={'train_runtime': 94.3653, 'train_samples_per_second': 116.611, 'train_steps_per_second': 14.592, 'total_flos': 405626802939840.0, 'train_loss': 0.32450293802054964, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ff7e6c-85ab-4a1f-85ea-df40b673a63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b02ce5da-dbee-4d4c-a990-c79ec54be556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0,\n",
       " 'input_ids': [101,\n",
       "  2572,\n",
       "  3217,\n",
       "  5831,\n",
       "  5496,\n",
       "  2010,\n",
       "  2567,\n",
       "  1010,\n",
       "  3183,\n",
       "  2002,\n",
       "  2170,\n",
       "  1000,\n",
       "  1996,\n",
       "  7409,\n",
       "  1000,\n",
       "  1010,\n",
       "  1997,\n",
       "  9969,\n",
       "  4487,\n",
       "  23809,\n",
       "  3436,\n",
       "  2010,\n",
       "  3350,\n",
       "  1012,\n",
       "  102,\n",
       "  7727,\n",
       "  2000,\n",
       "  2032,\n",
       "  2004,\n",
       "  2069,\n",
       "  1000,\n",
       "  1996,\n",
       "  7409,\n",
       "  1000,\n",
       "  1010,\n",
       "  2572,\n",
       "  3217,\n",
       "  5831,\n",
       "  5496,\n",
       "  2010,\n",
       "  2567,\n",
       "  1997,\n",
       "  9969,\n",
       "  4487,\n",
       "  23809,\n",
       "  3436,\n",
       "  2010,\n",
       "  3350,\n",
       "  1012,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f03f676e-87bc-409c-9049-f5b432013f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 408\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "341fae1f-99bf-44a7-ae79-a5ff3336c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (1.24.1)\n",
      "Requirement already satisfied: dill in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from evaluate) (23.2)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.1/84.1 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50058a1f-f706-484b-bcb4-a6758901f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (1.24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f98669b6-fd48-4ad0-bbc1-4dfdd8c90bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define compute metrics\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b690fc96-d733-489d-be23-2159bb6ffbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# redefine the training arguments, how metrics is computed and where is the directory\n",
    "train_args = TrainingArguments(\"train_classification1\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a09d5320-504b-4a4a-9a6c-6f2d8713dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.11.4-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.4/60.4 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\fict\\documents\\limjq_llm\\myenv\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp310-cp310-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.3 MB 15.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.0/9.3 MB 25.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.7/9.3 MB 37.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.3 MB 45.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 42.4 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.4-cp310-cp310-win_amd64.whl (44.1 MB)\n",
      "   ---------------------------------------- 0.0/44.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.9/44.1 MB 63.0 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 5.2/44.1 MB 55.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 7.2/44.1 MB 51.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 9.1/44.1 MB 48.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 10.4/44.1 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 11.4/44.1 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 12.4/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 13.5/44.1 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 14.5/44.1 MB 28.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 15.5/44.1 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.5/44.1 MB 25.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.5/44.1 MB 24.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 18.6/44.1 MB 24.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 19.6/44.1 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 20.6/44.1 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 21.6/44.1 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 22.7/44.1 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.7/44.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 24.8/44.1 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.9/44.1 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 27.0/44.1 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 28.1/44.1 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 29.3/44.1 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 30.4/44.1 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.5/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 32.6/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.8/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.9/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.0/44.1 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.2/44.1 MB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.3/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.5/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.7/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.8/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.8/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.6/44.1 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 22.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 22.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.1/44.1 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.11.4 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bad9b0de-58a2-452a-ac5c-e0e692e42d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.406799</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.901060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.712412</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.878472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.892179</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.888514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.2069919228813487, metrics={'train_runtime': 102.5504, 'train_samples_per_second': 107.303, 'train_steps_per_second': 13.428, 'total_flos': 541038552024960.0, 'train_loss': 0.2069919228813487, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7768525-6f62-406e-8fbf-37fec6923927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1]['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f68a583-65f1-4226-aa5c-f79e56e4602c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac956016-7e78-4952-9ed9-c3a24ef4547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2066, 6207, 1012,  102, 1045, 2293, 6207, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new prediction (custom data)\n",
    "new_data = tokenizer(\"I like apple.\", \"I love apple.\", truncation=True, return_tensors=\"pt\")\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a6a390f-4d07-4491-a40c-0aeea2595b8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\limjq_llm\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:266\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cuda'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m())\n",
      "File \u001b[1;32m~\\Documents\\limjq_llm\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:268\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffdeae-f6b1-453f-bdb5-e8f511c1ff5a",
   "metadata": {},
   "source": [
    "# full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90643b64-6b48-4cc5-97d0-3728302029aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████| 1725/1725 [00:00<00:00, 20395.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load the dataset and define the tokenizer\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "# tokenized data and data collator\n",
    "tokenized_dataset = raw_datasets.map(tokenize_func, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cc38a-a57e-450d-b131-688246e45aa8",
   "metadata": {},
   "source": [
    "## Post processing of the tokenize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b1f14df-1c04-48ad-8acc-f22ff954fe13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'validation': ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'test': ['labels', 'input_ids', 'token_type_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe6fd60d-513e-48da-ae7c-36640ef3bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"], collate_fn=data_collator, batch_size=8 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55c39f55-82fa-4781-bf29-235b77516b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 70]),\n",
       " 'token_type_ids': torch.Size([8, 70]),\n",
       " 'attention_mask': torch.Size([8, 70])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "{k:v.shape for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e7318ee-f4b3-4755-8003-8aa170cff292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2ee0292-e23e-4963-891d-eedae451d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6582, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afb9df98-db59-44d5-bddc-3619d103b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fict\\Documents\\limjq_llm\\myenv\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cf6d000-7c71-47b5-bbf4-8e7433b208ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for cuda (if there is GPU acceleration)\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "797faa07-87be-4981-99e8-01fae6ef057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 1376/1377 [01:23<00:00, 16.85it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3b25aa4-d5d8-4546-b56d-d2e46b7e6ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8651960784313726, 'f1': 0.9043478260869565}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1377/1377 [01:41<00:00, 16.85it/s]"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dbaa9-1781-4c02-a24b-3e340b7ee177",
   "metadata": {},
   "source": [
    "# New task (token classification)\n",
    "\n",
    "## ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed2878b5-f640-48ce-a4f3-41142d7057e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading builder script: 100%|█████████████████████████████████████████████████| 9.57k/9.57k [00:00<00:00, 9.56MB/s]\u001b[A\n",
      "\n",
      "Downloading metadata: 100%|███████████████████████████████████████████████████████████████| 3.73k/3.73k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "Downloading readme: 100%|█████████████████████████████████████████████████████████| 12.3k/12.3k [00:00<00:00, 13.1MB/s]\u001b[A\n",
      "\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████| 983k/983k [00:00<00:00, 9.96MB/s]\u001b[A\n",
      "\n",
      "Generating train split:   0%|                                                         | 0/14041 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating train split:   8%|███▏                                       | 1054/14041 [00:00<00:01, 10260.38 examples/s]\u001b[A\n",
      "Generating train split:  17%|███████▌                                    | 2400/14041 [00:00<00:01, 9281.98 examples/s]\u001b[A\n",
      "Generating train split:  27%|███████████▉                                | 3824/14041 [00:00<00:01, 9320.59 examples/s]\u001b[A\n",
      "Generating train split:  35%|███████████████▎                            | 4892/14041 [00:00<00:00, 9697.16 examples/s]\u001b[A\n",
      "Generating train split:  43%|██████████████████▊                         | 6000/14041 [00:00<00:00, 9946.00 examples/s]\u001b[A\n",
      "Generating train split:  54%|███████████████████████▌                    | 7520/14041 [00:00<00:00, 9952.06 examples/s]\u001b[A\n",
      "Generating train split:  64%|████████████████████████████▏               | 8987/14041 [00:00<00:00, 9840.45 examples/s]\u001b[A\n",
      "Generating train split:  73%|███████████████████████████████▍           | 10269/14041 [00:01<00:00, 9348.76 examples/s]\u001b[A\n",
      "Generating train split:  83%|███████████████████████████████████▋       | 11649/14041 [00:01<00:00, 9248.69 examples/s]\u001b[A\n",
      "Generating train split:  93%|███████████████████████████████████████▊   | 13000/14041 [00:01<00:00, 9009.88 examples/s]\u001b[A\n",
      "Generating train split: 100%|███████████████████████████████████████████| 14041/14041 [00:01<00:00, 9285.60 examples/s]\u001b[A\n",
      "\n",
      "Generating validation split:   0%|                                                     | 0/3250 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating validation split:  31%|████████████▎                           | 1000/3250 [00:00<00:00, 8520.45 examples/s]\u001b[A\n",
      "Generating validation split:  59%|███████████████████████▍                | 1902/3250 [00:00<00:00, 8712.11 examples/s]\u001b[A\n",
      "Generating validation split: 100%|████████████████████████████████████████| 3250/3250 [00:00<00:00, 8341.12 examples/s]\u001b[A\n",
      "\n",
      "Generating test split:   0%|                                                           | 0/3453 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating test split:  30%|█████████████▍                               | 1030/3453 [00:00<00:00, 10125.14 examples/s]\u001b[A\n",
      "Generating test split:  64%|█████████████████████████████▌                | 2220/3453 [00:00<00:00, 8538.83 examples/s]\u001b[A\n",
      "Generating test split: 100%|██████████████████████████████████████████████| 3453/3453 [00:00<00:00, 9319.05 examples/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c885c24-fc4f-4012-b6bd-2cad3b5227d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c0dba6b-27c7-4104-8d65-f5b8ad97f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "[22, 42, 16, 21, 35, 37, 16, 21, 7]\n",
      "[11, 21, 11, 12, 21, 22, 11, 12, 0]\n",
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "idx_sample = 0\n",
    "print(raw_datasets[\"train\"][idx_sample][\"id\"])\n",
    "print(raw_datasets[\"train\"][idx_sample][\"tokens\"])\n",
    "print(raw_datasets[\"train\"][idx_sample][\"pos_tags\"])\n",
    "print(raw_datasets[\"train\"][idx_sample][\"chunk_tags\"])\n",
    "print(raw_datasets[\"train\"][idx_sample][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8111101-a1e0-476e-9a1f-47b04c106424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_features = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b857137a-5256-4f23-973f-6d5cf15b5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_features = raw_datasets[\"train\"].features[\"chunk_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1427e288-1c38-4cd4-a6de-c98e88949d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0a1da51-0269-4f84-8694-888b3a3f7c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_features.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c3275b8-6b91-4d5d-b3e8-64d0ee2201ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU    rejects German call to boycott British lamb . \n",
      "B-ORG O       B-MISC O    O  O       B-MISC  O    O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][idx_sample][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][idx_sample][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79305b-8925-4f9b-98f4-8d0069c307a2",
   "metadata": {},
   "source": [
    "**Warning⚠️**\n",
    "---\n",
    "We have to pass a specific argument to `tokenizer` API when we are dealing with pre-tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96584e0d-b8db-4527-be2e-030985d678b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 29.0/29.0 [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "vocab.txt:   0%|                                                                            | 0.00/213k [00:00<?, ?B/s]\u001b[A\n",
      "vocab.txt: 100%|█████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 422kB/s]\u001b[A\n",
      "\n",
      "tokenizer.json:   0%|                                                                       | 0.00/436k [00:00<?, ?B/s]\u001b[A\n",
      "tokenizer.json: 100%|████████████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 590kB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd9f6d09-98c1-431b-817d-07874599f39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e367c29c-5ca5-4429-87b9-23dbd6f5a635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][idx_sample][\"tokens\"], is_split_into_words=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0503a97f-902b-4c8a-a7ed-8e3ffa950aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a06144fb-551f-4853-abc1-88e7e9577871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016d98c-a290-4a11-8d3d-94837027c654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
